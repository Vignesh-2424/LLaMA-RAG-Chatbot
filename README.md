# ğŸ§  LLaMA RAG Chatbot

A full-stack AI chatbot that allows users to:

- âœ… Upload multiple PDF or text documents  
- â“ Ask questions related to those documents (RAG-based answers)  
- ğŸ’¬ Ask general questions answered by the LLaMA model (via Ollama)

Built using **FastAPI** (backend) and **React** (frontend), integrated with **Ollama**, **LangChain**, and **FAISS** for retrieval-augmented generation.

---

## ğŸš€ Features

- ğŸ—‚ï¸ Upload multiple documents (PDF / TXT)
- ğŸ“š Vector-based semantic search using FAISS
- ğŸ’¡ RAG-powered Q&A from uploaded documents
- ğŸŒ General LLM response fallback
- ğŸ¨ Stylish, responsive UI
- ğŸ”¥ Single-command launch for frontend & backend

---

## ğŸ§± Tech Stack

- **Frontend**: React, HTML/CSS
- **Backend**: FastAPI, Python
- **Vector DB**: FAISS
- **LLM + Embeddings**: Ollama (LLaMA 3)
- **Document Parsing**: PyMuPDF
- **Embeddings/Chain**: LangChain
- **Multi-process Launcher**: Concurrently (Node)

---

## ğŸ“¦ Requirements

### âœ… Backend (Python)

Install in a virtual environment:

```bash
pip install fastapi uvicorn langchain faiss-cpu ollama PyMuPDF pydantic python-multipart
```

### âœ… Frontend (Node)

```bash
npm install
```

### âœ… Multi-process runner

From the project root:

```bash
npm init -y
npm install concurrently
```

---

## ğŸ“ Project Structure

```
llama-rag-chatbot/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ document_handler.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ public/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ vector_store/
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

---

## ğŸ”§ Scripts (`package.json`)

```json
"scripts": {
  "start": "concurrently \"npm run server\" \"npm run client\"",
  "server": "cd backend && venv\\Scripts\\activate && uvicorn main:app --reload",
  "client": "cd frontend && npm start"
}
```

> ğŸ’¡ On Linux/Mac:
```json
"server": "cd backend && source venv/bin/activate && uvicorn main:app --reload"
```

---

## ğŸ§ª How to Run (Single Command)

From the root folder:

```bash
npm start
```

This will:

* â© Start the FastAPI backend with `uvicorn`
* â© Launch the React frontend
* âš™ï¸ Connect both seamlessly for local development

---

## ğŸ“¸ Demo

> Add your screenshots in a `screenshots/` folder if you'd like.

Example:

![Upload Demo](./screenshots/upload.png)  
![Chat Demo](./screenshots/chat.png)

---

## ğŸ“„ License

MIT â€“ Use freely, but attribution appreciated.

---

## ğŸ™‹â€â™‚ï¸ Author

Made with â¤ï¸ by **Vignesh R.S**
